\documentclass[12pt]{article}

\usepackage{verbatim}
\usepackage{apalike}
\let\bibhang\relax
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{url}
\usepackage[colorlinks = true]{hyperref}
\usepackage[left=1.0cm,right=1cm,top=3cm,bottom=3cm]{geometry}
\usepackage{amsfonts,amssymb}
\usepackage{mathtools}

\def\labelitemi{--}

\title{Pedantic description of the svGPFA theory}

\author{Joaqu\'{i}n Rapela}

\begin{document}

\maketitle

\abstract{Here we provide a pedantic description of the sparse variational
Gaussian process factor analysis (svGPFA) theory, expanding on
\cite{dunckerAndSahani18}.}

\begin{comment}

\section{GPFA model}

\begin{equation}
    \begin{aligned}
        p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}) &= \prod_{r=1}^R\prod_{k=1}^Kp(x_{kr}(\cdot))\\
        x_{kr}(\cdot) &\sim \mathcal{GP}(\mu_k(\cdot),\kappa_k(\cdot,\cdot))&&{\text{for}\; k=1, \ldots, K\;\text{and}\;r=1,\ldots, R}\\
        h_{nr}(\cdot) &= \sum_{k=1}^Kc_{nk}x_{kr}(\cdot) + d_n&&{\text{for}\; n=1, \ldots, N\;\text{and}\;r=1,\ldots, R}\\
        p(\{\mathbf{y}_{nr}\}_{n=1,r=1}^{N,R}|\{h_{nr}(\cdot)\}_{n=1,r=1}^{N,R}) &= \prod_{r=1}^R\prod_{n=1}^Np(\mathbf{y}_{nr}|h_{nr}(\cdot))
    \end{aligned}
    \label{eq:gpfaModel}
\end{equation}

\noindent where $x_{kr}(\cdot)$ is the latent process $k$ in trial $r$, $h_{nr}(\cdot)$ is the embedding process for neuron $n$ and trial $r$ and $\mathbf{y}_{nr}$ is the activity of neuron $n$ in trial $r$.

Notes:

\begin{itemize}

    \item the first equation shows that the latent processes are independent,

    \item the second equation shows that the latent processes share mean and
        covariance functions across trials. That is, for any $k$, the mean and
        covariance functions  of latents processes of different trials,
        $x_{kr}(\cdot), r=1,\ldots, R$, are the same ($\mu_k(\cdot)$ and
        $\kappa_k(\cdot,\cdot)$),

    \item the fourth equation shows that, given the embedding processes, the
        responses of different neurons are independent.

\end{itemize}

\end{comment}

\section{svGPFA prior}

To use the sparse variational framework for Gaussian processes \citet{dunckerAndSahani18} augmented the GPFA model by introducing inducing
points $\mathbf{u}_{kr}$ for each latent process $k$ and trial $r$. The
inducing points $\mathbf{\mathbf{u}}_{kr}$ represent evaluations of the latent
process $x_{kr}(\cdot)$ at the $M_{kr}$ locations $\mathbf{\mathbf{z}}_{kr}$. A
joint prior over the latent process $x_{kr}(\cdot)$ and its inducing points
$\mathbf{u}_{kr}$ is given in Eq.~\ref{eq:gpfaWithIndPointsPrior}.

\begin{equation}
    \begin{aligned}
        p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R},\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R}) &= p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}|\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R})p(\{\mathbf{u}_{kr}\}_  {k=1,r=1}^{K,R})\\
        p(\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}|\{\mathbf{u}_{kr}\}_  {k=1,r=1}^{K,R}) &= \prod_{k=1}^k\prod_{r=1}^{R}p(x_{kr}(\cdot)|\mathbf{u}_{kr})\\
        p(\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R})&=\prod_{k=1}^k\prod_{r=1}^{R}p(\mathbf{u}_{kr})\\
        p(\mathbf{u}_{kr})&=\mathcal{N}(\mathbf{0},K^{kr}_{zz})
    \end{aligned}
    \label{eq:gpfaWithIndPointsPrior}
\end{equation}

\noindent where $K_{zz}^{(kr)}[i,j]=\kappa_k(\mathbf{z}_{kr}[i],\mathbf{z}_{kr}[j])$.

We next derive the functional form of $p(x_{kr}(\cdot)|\mathbf{u}_{kr})$.

For any $P\in\mathbb{N}$, define the random vector $\mathbf{x}_{kr}$ as the random process
$x_{kr}(\cdot)$ evaluated at times
$\mathbf{t}_P^{(r)}\coloneqq\left\{t_1^{(r)},\ldots,t_P^{(r)}\right\}$. That
is,

\begin{equation}
    \mathbf{x}_{kr}\coloneqq[x_{kr}(t_1^{(r)}),\ldots,x_{kr}(t_P^{(r)})]^\intercal
    \label{eq:latentVector}
\end{equation}

Then, because the inducing points $\mathbf{u}_{kr}$ are evaluations of the
latent process $x_{kr}(\cdot)$ at $\mathbf{z}_{kr}$, $\mathbf{x}_{kr}$ and
$\mathbf{u}_{kr}$ are jointly Gaussian:

\begin{equation}
    p\left(\left[\begin{array}{c}
        \mathbf{u}_{kr}\\
        \mathbf{x}_{kr}
    \end{array}\right]\right)
    =\mathcal{N}\left(\left.\left[\begin{array}{c}
        \mathbf{u}_{kr}\\
        \mathbf{x}_{kr}
    \end{array}\right]\right|\left[\begin{array}{c}
        \mathbf{0}\\
        \mathbf{0}
    \end{array}\right],\left[\begin{array}{cc}
        K_\mathbf{zz}^{(kr)}&K_\mathbf{zt}^{(kr)}\\
        K_\mathbf{tz}^{(kr)}&K_\mathbf{tt}^{(r)}
    \end{array}\right]\right)
    \label{eq:prior}
\end{equation}

\noindent where
$K_\mathbf{tz}^{(kr)}[i,j]=\kappa_k(t^{(r)}_i,\mathbf{z}_{kr}[j])$,
$K_\mathbf{zt}^{(kr)}[i,j]=\kappa_k(\mathbf{z}_{kr}[i],t_j^{(r)})$
and
$K_\mathbf{tt}^{(r)}[i,j]=\kappa_k(t_i^{(r)},t_j^{(r)})$.

Now, applying the formula for the conditional pdf of jointly Normal random
vectors~\citep[][Eq. 2.116]{bishop06} to Eq.~\ref{eq:prior}, we obtain

\begin{equation}
    p(\mathbf{x}_{kr}|\mathbf{u}_{kr})=\mathcal{N}\left(\mathbf{x}_{kr}\left|K_\mathbf{tz}^{(kr)}\left(K_{zz}^{(kr)}\right)^{-1}\mathbf{u}_{kr},\;K_\mathbf{tt}^{(r)}-K_\mathbf{tz}^{(kr)}\left(K_{zz}^{(kr)}\right)^{-1}K_\mathbf{zt}^{(kr)}\right.\right)
    \label{eq:priorLatentsGivenIndPointsVector}
\end{equation}

Because Eq.~\ref{eq:priorLatentsGivenIndPointsVector} is valid for any
$\mathbf{t}_P^{(r)}$, and any $\mathbf{x}_{kr}$ derived from it, it follows that

\begin{equation}
    \begin{aligned}
        p(x_{kr}(\cdot)|\mathbf{u}_{kr})&=\mathcal{GP}\left(\tilde{\mu}_{kr}(\cdot),\tilde{\kappa}_{kr}(\cdot,\cdot\right))\\
        \tilde{\mu}_{kr}(t)&=\kappa_k(t,\mathbf{z}_{kr})\left(K_{zz}^{(kr)}\right)^{-1}\mathbf{u}_{kr}\\
        \tilde{\kappa}_k(t,t')&=\kappa_k(t,t')-\kappa_k(t,\mathbf{z}_{kr})\left(K_{zz}^{(kr)}\right)^{-1}\kappa_k(\mathbf{z}_{kr},t')
        \label{eq:priorLatentsGivenIndPoints} 
    \end{aligned}
\end{equation}

\noindent which is Eq.~3 in \citet{dunckerAndSahani18}.

We obtain the svGPFA prior on the latents by using the marginal formula of the
linear Gaussian model \citep[][Eq.~2.115]{bishop06} with the last line in
Eq.~\ref{eq:prior} and with Eq.~\ref{eq:priorLatentsGivenIndPointsVector},
yielding

\begin{equation}
    p(\mathbf{x}_{kr})=\mathcal{N}\left(\mathbf{x}_{kr}\left|\mathbf{0},\;K_\mathbf{tt}^{(r)}\right.\right)
    \label{eq:latentsPriorVector}
\end{equation}

Because Eq.~\ref{eq:priorLatentsGivenIndPointsVector} is valid for any
$\mathbf{t}_P^{(r)}$, and any $\mathbf{x}_{kr}$ derived from it, it follows that

\begin{equation*}
    p(x_{kr}(\cdot))=\mathcal{GP}\left(0(\cdot),\;\kappa_k(\cdot,\cdot)\right)
\end{equation*}

\noindent which shows that the svGPFA prior on the latent processes is
unrelated to the inducing points.

\section{Approximate posterior on the latent process $x_{kr}(\cdot)$}

From Eq.~3 in \citet[][supplementary]{dunckerAndSahani18}, the approximate joint
posterior for the latent process $x_{kr}(\cdot)$ and the inducing points
$\mathbf{u}_{kr}$ is

\begin{equation}
    q(x_{kr}(\cdot),\mathbf{u}_{kr})=p(x_{kr}(\cdot)|\mathbf{u}_{kr})q(\mathbf{u}_{kr})
\end{equation}

\noindent with $p(x_{kr}(\cdot)|\mathbf{u}_{kr})$ given in Eq.~\ref{eq:priorLatentsGivenIndPoints} and

\begin{equation}
    q(\mathbf{u}_{kr})=\mathcal{N}(\mathbf{u}_{kr}(\cdot)|\mathbf{m}_{kr}, S_k)
    \label{eq:qu}
\end{equation}

Thus, for any random vector $\mathbf{x}_{kr}$, as in Eq.~\ref{eq:latentVector},
we have

\begin{equation}
    q(\mathbf{x}_{kr},\mathbf{u}_{kr})=p(\mathbf{x}_{kr}|\mathbf{u}_{kr})q(\mathbf{u}_{kr})
\end{equation}

\noindent with $p(\mathbf{x}_{kr}|\mathbf{u}_{kr})$ given in
Eq.~\ref{eq:priorLatentsGivenIndPointsVector}. Using the marginal formula for
the linear Gaussian model \citep[][Eq.~2.115]{bishop06} with
Eqs.~\ref{eq:priorLatentsGivenIndPointsVector} and~\ref{eq:qu} we obtain

\begin{align}
    q(\mathbf{x}_{kr})=\mathcal{N}\left(\mathbf{x}_{kr}|K_{tz}^{kr}(K_{zz}^{kr})^{-1}\mathbf{m}_{kr},\;K_{tt}^k+K_{tz}^{kr}\left((K_{zz}^{kr})^{-1}S_{kr}(K_{zz}^{kr})^{-1}-(K_{zz}^{kr})^{-1  }\right)K_{zt}^{kr}\right)
    \label{eq:qxRandomVec}
\end{align}

Because Eq.~\ref{eq:qxRandomVec} is valid for any $\mathbf{t}_P^{(r)}$, and any
$\mathbf{x}_{kr}$ derived from it, it follows that


\begin{equation}
    \begin{aligned}
        q(x_{kr}(\cdot))&=\mathcal{GP}\left(\breve\mu_{kr}(\cdot),\breve\kappa_{kr}(\cdot,\cdot)\right)\\
        \breve\mu_{kr}(t)&=\kappa_k(t,z_{kr})(K_{zz}^{kr})^{-1}\mathbf{m}_{kr},\\
        \breve\kappa_{kr}(t,t')&=\kappa_k(t,t')+\kappa_k(t,z_{kr})\left((K_{zz}^{kr})^{-1}S_{kr}(K_{zz}^{kr})^{-1}-(K_{zz}^{kr})^{-1}\right)\kappa_k(z_{kr},t')
    \end{aligned}
    \label{eq:qx}
\end{equation}

\section{Approximate posterior on the embedding $h_{nr}(\cdot)$}

The embedding for neuron $n$ and trial $r$ is

\begin{equation}
    h_{nr}(\cdot) = \sum_{k=1}^Kc_{nk}x_{kr}(\cdot)
    \label{eq:embedding_nr}
\end{equation}

Affine transformations of Gaussian processes are Gaussian processes, therefore
the approximate posterior on the embedding
$h_{nr}(\cdot)$ is a Gaussian process. That is,

\begin{equation}
    q(h_{nr}(\cdot))=q\left(\sum_{k=1}^Kc_{nk}x_{kr}(\cdot)\right)=\mathcal{GP}(\tilde{\mu}_{nr}(\cdot),\tilde{\kappa}_{nr}(\cdot,\cdot))
\end{equation}

\noindent with

\begin{equation}
    \begin{aligned}
        \tilde{\mu}_{nr}(t)&=E_q\{h_{nr}(t)\}=\sum_{k=1}^Kc_{nk}E_q\{x_{kr}(t)\}=\sum_{k=1}^Kc_{nk}\,\breve\mu_{kr}(t)\\
        \tilde{\kappa}_{nr}(t,t')&=E_q\left\{\left(h_{nr}(t)-\tilde{\mu}_{nr}(t)\right)\left(h_{nr}(t')-\tilde{\mu}_{nr}(t')\right)\right\}\\
                                 &=E_q\left\{\left(\sum_{k=1}^Kc_{nk}(x_{kr}(t)-\breve\mu_{kr}(t))\right)\left(\sum_{k'=1}^Kc_{nk'}(x_{k'r}(t')-\breve\mu_{k'r}(t'))\right)\right\}\\
                                 &=\sum_{k=1}^K\sum_{k'=1}^Kc_{nk}c_{nk'}E_q\left\{(x_{kr}(t)-\breve\mu_{kr}(t))(x_{k'r}(t')-\breve\mu_{k'r}(t'))\right\}\\
                                 &=\sum_{k=1}^Kc_{nk}^2E_q\left\{(x_{kr}(t)-\breve\mu_{kr}(t))(x_{kr}(t')-\breve\mu_{kr}(t'))\right\}\\
                                 &=\sum_{k=1}^Kc_{nk}^2\breve\kappa(t,t')
    \end{aligned}
    \label{eq:qh}
\end{equation}

The second-to-last line in Eq.~\ref{eq:qh} follows from the independence
assumption of the latent processes, $x_{kr}(\cdot)$ and $x_{k'r}(\cdot)$, $k\ne
k'$, by the variational distribution $q$
\citep[][supplementary]{dunckerAndSahani18}.

\section{Derivation of the svGPFA variational lower bound}

To derive the sparse variational lower bound we have the freedom of using
$\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}$ or $\{h_{nr}(\cdot)\}_{n=1,r=1}^{N,R}$ as
latent processes in the complete-data likelihood\footnote{To simplify notation below we write
$\{x_{kr}(\cdot)\}_{k=1,r=1}^{K,R}$ as $\{x_{kr}(\cdot)\}$ and similarly for $\{\mathbf{u}_{kr}\}_{k=1,r=1}^{K,R}$ and $\{h_{nr}(\cdot)\}_{n=1,r=1}^{N,R}$}. In
\cite[][supplementary]{dunckerAndSahani18} the authors used the former, which
requires the non-trivial proof of Eq.~\ref{eq:difficultProofDS18}.

\begin{align}
    \text{E}_{q(\{x_{kr}(\cdot)\})}\log p(\{\mathbf{y}_{nr}\}|\{x_{kr}(\cdot)\})=\text{E}_{q(\{h_{nr}(\cdot)\})}\log p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\})
    \label{eq:difficultProofDS18}
\end{align}

\noindent Here we use the latter, which avoids the need of Eq.~\ref{eq:difficultProofDS18}. Then the complete data likelihood is given in Eq.~\ref{eq:completeDataLikelihood} and a variational lower bound in Eq.~\ref{eq:lowerBound}.

\begin{align}
    p(\{\mathbf{y}_{nr}\}, \{h_{nr}(\cdot)\}, \{\mathbf{u}_{kr}\})=p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\})p(\{h_{nr}(\cdot)\}|\{\mathbf{u}_{kr}\})p(\{\mathbf{u}_{kr}\})
    \label{eq:completeDataLikelihood}
\end{align}

\begin{align}
    \log p(\{\mathbf{y}_{nr}\})\ge E_{q(\{h_{nr}(\cdot)\}, \{\mathbf{u}_{kr}\})}\log\left(p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\}, \{\mathbf{u}_{kr}\}\right)-KL\left\{q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})||p(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})\right\}
    % \log p(\{\mathbf{y}_{nr}\})\ge\int\int q(\{h_{nr}(\cdot)\}, \{\mathbf{u}_{kr}\})\log\left(\frac{p(\{\mathbf{y}_{nr}\}, \{h_{nr}(\cdot)\}, \{\mathbf{u}_{kr}\})}{q(\{h_{nr}(\cdot)\}, \{\mathbf{u}_{kr}\})}\right)d\{\mathbf{u}_{kr}\}d\{h_{nr}(\cdot)\}
    \label{eq:lowerBound}
\end{align}

As in \cite[][supplementary]{dunckerAndSahani18}, we choose the approximating variational distribution in Eq.~\ref{eq:approxDistr}.

\begin{equation}
    \begin{aligned}
        q(\{x_{kr}(\cdot)\}, \{\mathbf{u}_{kr}\})&=q(\{x_{kr}(\cdot)\}|\{\mathbf{u}_{kr}\})q(\{\mathbf{u}_{kr}\})\\
        q(\{x_{kr}(\cdot)\}|\{\mathbf{u}_{kr}\})&=p(\{x_{kr}(\cdot)\}|\{\mathbf{u}_{kr}\})\\
        q(\{\mathbf{u}_{kr}\})&=\prod_{r=1}^R\prod_{k=1}^Kq(\mathbf{u}_{kr})\\
        q(\mathbf{u}_{kr})&=\mathcal{N}(\mathbf{u}_{kr}|\mathbf{m}_{kr},S_{kr})
    \end{aligned}
    \label{eq:approxDistr}
\end{equation}

Because $h_{nr}(\cdot)$ is a function of $\{x_{kr}(\cdot)\}$ only (Eq.~\ref{eq:gpfaModel}), and because $q(\{x_{kr}(\cdot)\}|\{\mathbf{u}_{kr}\})=p(\{x_{kr}(\cdot)\}|\{\mathbf{u}_{kr}\})$ (Eq.~\ref{eq:approxDistr}), then 

\begin{align}
    q(\{h_{nr}(\cdot)\}|\{\mathbf{u}_{kr}\})=p(\{h_{nr}(\cdot)\}|\{\mathbf{u}_{kr}\})
    \label{eq:qH_equals_pH}
\end{align}

% \pagebreak
The first term in Eq.~\ref{eq:lowerBound} can be rewritten as

\begin{equation}
    \begin{aligned}
        &E_{q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})}\log\left(p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)=\\
        &\int\int q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})\log\left(p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\}\right)d\{\mathbf{u}_{kr}\}d\{h_{nr}(\cdot)\}=\\
        &\int\int q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})\log\left(p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\}\right)d\{\mathbf{u}_{kr}\}d\{h_{nr}(\cdot)\}=\\
        &\int\left(\int q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})d\{\mathbf{u}_{kr}\}\right)\log\left(p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\}\right)d\{h_{nr}(\cdot)\}=\\
        &\int q(\{h_{nr}(\cdot)\})\log\left(p(\{\mathbf{y}_{nr}\}|\{h_{nr}(\cdot)\}\right)d\{h_{nr}(\cdot)\}=\\
        &\int q(\{h_{nr}(\cdot)\})\log\left(\prod_{r=1}^R\prod_{n=1}^Np(\mathbf{y}_{nr}|h_{nr}(\cdot))\right)d\{h_{nr}(\cdot)\}=\\
        &\sum_{r=1}^R\sum_{n=1}^N\int q(\{h_{nr}(\cdot)\})\log\left(p(\mathbf{y}_{nr}|h_{nr}(\cdot))\right)d\{h_{nr}(\cdot)\}=\\
        &\sum_{r=1}^R\sum_{n=1}^N\int \int q(\{h_{n'r'}(\cdot)\}_{n'\ne n, r'\ne r}|h_{nr}(\cdot))q(h_{nr}(\cdot))\log\left(p(\mathbf{y}_{nr}|h_{nr}(\cdot))\right)d\{h_{n'r'}(\cdot)\}_{n'\ne n, r'\ne r}dh_{nr}(\cdot)=\\
        &\sum_{r=1}^R\sum_{n=1}^N\int\left(\int q(\{h_{n'r'}(\cdot)\}_{n'\ne n, r'\ne r}|h_{nr}(\cdot))d\{h_{n'r'}(\cdot)\}_{n'\ne n,r'\ne r}\right)q(h_{nr}(\cdot))\log\left(p(\mathbf{y}_{nr}|h_{nr}(\cdot))\right)dh_{nr}(\cdot)=\\
        &\sum_{r=1}^R\sum_{n=1}^N\int q(h_{nr}(\cdot))\log\left(p(\mathbf{y}_{nr}|h_{nr}(\cdot))\right)dh_{nr}(\cdot)=\\
        &\sum_{r=1}^R\sum_{n=1}^NE_{q(h_{nr}(\cdot))}\log\left(p(\mathbf{y}_{nr}|h_{nr}(\cdot))\right)
    \end{aligned}
   \label{eq:expectedLogLike}
\end{equation}

Notes:

\begin{itemize}

    \item the second line follows from the previous one because, given the
        embedding process, activities of neurons are independent of inducing
        points (Eq.~\ref{eq:completeDataLikelihood}),

    \item the fifth line follows from the previous one because, given embedding
        processes, responses of neurons are independent of each other (last
        line in Eq.~\ref{eq:gpfaModel}),

\end{itemize}

\pagebreak
The second term in Eq.~\ref{eq:lowerBound} can be rewritten as

\begin{equation}
    \begin{aligned}
        KL\left\{q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})||p(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})\right\}&=\int\int q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})\log\frac{q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})}{p(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})}d\{h_{nr}(\cdot)\}d\{\mathbf{u}_{kr}\}\\
        &=\int\int q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})\log\frac{q(\{h_{nr}(\cdot)\}|\{\mathbf{u}_{kr}\})q(\{\mathbf{u}_{kr}\})}{p(\{h_{nr}(\cdot)\}|\{\mathbf{u}_{kr}\})p(\{\mathbf{u}_{kr}\})}d\{h_{nr}(\cdot)\}d\{\mathbf{u}_{kr}\}\\
        &=\int\int q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})\log\frac{q(\{\mathbf{u}_{kr}\})}{p(\{\mathbf{u}_{kr}\})}d\{h_{nr}(\cdot)\}d\{\mathbf{u}_{kr}\}\\
        &=\int\int q(\{h_{nr}(\cdot)\},\{\mathbf{u}_{kr}\})d\{h_{nr}(\cdot)\}\log\frac{q(\{\mathbf{u}_{kr}\})}{p(\{\mathbf{u}_{kr}\})}d\{\mathbf{u}_{kr}\}\\
        &=\int q(\{\mathbf{u}_{kr}\})\log\frac{q(\{\mathbf{u}_{kr}\})}{p(\{\mathbf{u}_{kr}\})}d\{\mathbf{u}_{kr}\}\\
        &=\int q(\{\mathbf{u}_{kr}\})\log\frac{\prod_{r=1}^R\prod_{k=1}^Kq(\mathbf{u}_{kr})}{\prod_{r=1}^R\prod_{k=1}^Kp(\mathbf{u}_{kr})}d\{\mathbf{u}_{kr}\}\\
        &=\int q(\{\mathbf{u}_{kr}\})\sum_{r=1}^R\sum_{k=1}^K\log\frac{q(\mathbf{u}_{kr})}{p(\mathbf{u}_{kr})}d\{\mathbf{u}_{kr}\}\\
        &=\sum_{r=1}^R\sum_{k=1}^K\int q(\{\mathbf{u}_{kr}\})\log\frac{q(\mathbf{u}_{kr})}{p(\mathbf{u}_{kr})}d\{\mathbf{u}_{kr}\}\\
        &=\sum_{r=1}^R\sum_{k=1}^K\int\prod_{k'=1}^K\prod_{r'=1}^Rq(\mathbf{u}_{k'r'})\log\frac{q(\mathbf{u}_{kr})}{p(\mathbf{u}_{kr})}d\{\mathbf{u}_{kr}\}\\
        &=\sum_{r=1}^R\sum_{k=1}^K\int q(\mathbf{u}_{kr})\log\frac{q(\mathbf{u}_{kr})}{p(\mathbf{u}_{kr})}d\mathbf{u}_{kr}\\
        &=\sum_{r=1}^R\sum_{k=1}^K KL(q(\mathbf{u}_{kr})||p(\mathbf{u}_{kr}))
    \end{aligned}
    \label{eq:kl}
\end{equation}

Notes:

\begin{itemize}

    \item the third line follows from the previous one by Eq.~\ref{eq:qH_equals_pH},

    \item the tenth line follows from the previous because, for $k'\neq k$ or
        $r'\neq r$, the factors $q(\mathbf{u}_{k'r'})$ integrate out to one.

\end{itemize}

Replacing Eq.~\ref{eq:expectedLogLike} and Eq.~\ref{eq:kl} into Eq.~\ref{eq:lowerBound} we obtain

\begin{align}
    \log p(\{\mathbf{y}_{nr}\})\ge \sum_{r=1}^R\sum_{n=1}^NE_{q(h_{nr}(\cdot))}\log\left(p(\mathbf{y}_{nr}|h_{nr}(\cdot))\right)-\sum_{r=1}^R\sum_{k=1}^K KL(q(\mathbf{u}_{kr})||p(\mathbf{u}_{kr}))
    \label{eq:lowerBoundFinal}
\end{align}

\noindent which is Eq.~4 in \cite{dunckerAndSahani18}.

\bibliographystyle{apalike}
\bibliography{latentVariableModels,machineLearning}
\end{document}
